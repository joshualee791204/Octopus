{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Unleashing the mystery of Kaggle\n",
    "- How does it all work?\n",
    "- Feature Engineering\n",
    "- Parameter Tuning\n",
    "- Ensembling & Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it all work - Should I trust the public leaderboard?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can find how Kaggle calculate the public leaderboard [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/leaderboard).\n",
    "- If the competition has a large training set and a relatively small public test set compared to private test set, you can easily overfit the public data set. In this case, you **should not** trust the public leaderboard. \n",
    "- If the traing set and test set are collected from different time frames, you **must** trust the public leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://s3.amazonaws.com/nycdsabt01/s2-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV or LB?\n",
    "- **TRUST YOUR CV!**\n",
    "- Typical question on smaller datasets: \n",
    " - “I’m doing proper cross-validation and see improvements on my CV score, but public leaderboard is so random and does not correlate at all!”\n",
    "- Top kagglers’ pick most of the time:\n",
    " - Final Submission = $X*CV + (1-X)*LB$, typically $X=0.5$ is OK.\n",
    "- Trusting CV is a hard thing to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Structure the dataframes before we dive into the technical details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the 'Id' column\n",
    "train_ID = train_df['Id']\n",
    "test_ID = test_df['Id']\n",
    "\n",
    "# Now drop the 'Id' colum since we can not use it as a feature to train our model.\n",
    "train_df.drop(\"Id\", axis = 1, inplace = True)\n",
    "test_df.drop(\"Id\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = train_df['SalePrice']\n",
    "X_train = train_df.drop('SalePrice', axis=1)\n",
    "X_test = test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Delete the dataframes that you do not need anymore to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combine training and test dataframes before feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([X_train, X_test])\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering - most creative aspect of Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical  features\n",
    "- Nearly always need some treatment\n",
    "- High cardinality can create very sparse data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding\n",
    "- One-of-K encoding on an array of length K\n",
    "- Basic method: used with most linear algorithm\n",
    "- Drop first column avoids collinearity\n",
    " - encoding gender as two variables, **is_male** and **is_female**, produces two features which are perfectly negatively correlated\n",
    "- Encode categories appearing 3+ times\n",
    " - Reduce training feature space with no loss of info.\n",
    "- Will fail on new category. (See this [get_smarties](https://github.com/joeddav/get_smarties) package on Github for a quick fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in all_data.columns:\n",
    "    if all_data[c].dtype == 'object':\n",
    "        print(c, len(all_data[c].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_df = pd.get_dummies(all_data, drop_first=True, dummy_na=True)\n",
    "one_hot_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label encoding\n",
    "- Give every categorial variable a unique numerical ID\n",
    "- Useful for non-linear tree-based algorithm\n",
    "- Does not increase dimensionality\n",
    "- Will fail on new category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_df = all_data.copy()\n",
    "\n",
    "for c in label_df.columns:\n",
    "    if label_df[c].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        # Need to convert the column type to string in order to encode missing values\n",
    "        label_df[c] = le.fit_transform(label_df[c].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features with many categories - rows:category ratio 20:1 or less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Count encoding\n",
    "- Rank categorical variables by count in the **training** set and transform the test set\n",
    "- Iterate counter for each CV fold - fit on the **new training set** and transform on the **new test set**\n",
    "- Useful for both linear or non-linear algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabelCountEncoder(object):\n",
    "    def __init__(self):\n",
    "        self.count_dict = {}\n",
    "    \n",
    "    def fit(self, column):\n",
    "        # This gives you a dictionary with level as the key and counts as the value\n",
    "        count = column.value_counts().to_dict()\n",
    "        # We want to rank the key by its value and use the rank as the new value\n",
    "        self.count_dict = {key[0]: rank+1 for rank, key in enumerate(sorted(count.items(), key=lambda x: x[1]))}\n",
    "    \n",
    "    def trasnform(self, column):\n",
    "        # If a category only appears in the test set, we will assign the value to zero.\n",
    "        missing = 0\n",
    "        return column.apply(lambda x : self.count_dict.get(x, missing))\n",
    "    \n",
    "    def fit_transform(self, column):\n",
    "        self.fit(column)\n",
    "        return self.trasnform(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_count_df = X_train.copy()\n",
    "\n",
    "for c in label_count_df.columns:\n",
    "    if label_count_df[c].dtype == 'object':\n",
    "        lce = LabelCountEncoder()\n",
    "        label_count_df[c] = lce.fit_transform(label_count_df[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Interactions\n",
    "- If interactions are natural for a problem - ML only does approximations! => sub-optimal\n",
    " - Start from interactions that make sense intuitively. \n",
    " - Winners usually find something that most people struggle to see in data. **Not many people look at the data at all!**\n",
    " \n",
    "|  GarageCond |   GarageType   | GarageCond * GarageType  |\n",
    "| ------------|:--------------:| -----:|\n",
    "|  Ex  | 2Types | Ex * 2Types |\n",
    "|  Ex  | CarPort| Ex * CarPort|\n",
    "|  TA  | Basement| TA * Basement|\n",
    "|  Fa  | BuiltIn | Fa * BuiltIn |\n",
    " \n",
    " \n",
    "- Test your method with all explicitly created possible 2-way interactions if you have enough computing power\n",
    "- This is especially useful when dealing with **anonymous data** (column name unknown)\n",
    "- If 2-way interactions help – go even further (3-way, 4-way, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dealing with NA's depends on situation. NA itself is an information unit! Usually separate category is enough.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical features\n",
    "Feature transformations to consider:\n",
    "- Scaling - min/max, N(0,1), root/power scaling, log scaling, Box-Cox, quantiles.\n",
    " - **[Fit on the training set and transform on the test set.](https://stats.stackexchange.com/a/174865)**\n",
    "- Rounding (too much precision might be noise!)\n",
    "- Interactions {+,-,*,/}\n",
    " - Since area related features are very important to determine house prices, we can add one more feature which is the total area of basement, first and second floor areas of each house\n",
    " - `all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']`\n",
    "- Sometimes numerical features are categorical or even ordinal by nature.\n",
    " - `MSSubClass`, `OverallCond`, `OverallQual`, `YrSold`, `MoSold`\n",
    "- **Tree methods are almost invariant to scaling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Everything you need to know about feature engineering is [here](https://www.slideshare.net/HJvanVeen/feature-engineering-72376750?qid=14629b24-6d05-4275-acc9-ea0743605071&v=&b=&from_search=1).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic approach: apply grid search on all parameter space\n",
    "- Zero effort and no supervision\n",
    "- Enormous parameter space\n",
    "- Very time consuming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expert approach: experience + intuition + resources at hand\n",
    "1. Pick one set of parameters from the Kaggle kernel or the golden parameter you used in the previous competition\n",
    "2. Start with the parameter that doesn't affect the others too much\n",
    " - i.e. learning rate $\\eta $ in boosting method doesn't influence other parameter tuning (from my experience)\n",
    " - `max_depth`, `min_samples_split` and `min_samples_leaf` in random forest are highly correlated with each other\n",
    "3. Iteratively tuning the features that control overfitting/underfitting\n",
    " - If it helps on CV, try to tune it as much as possible. Stop after CV score converges.\n",
    " - You can use public leaderboard as your K+1 fold to further prove it.\n",
    "4. Go back to step 2 and stop when you are satisfied with the result and won't regret not working harder.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Bayesian optimization method](https://github.com/fmfn/BayesianOptimization/blob/master/examples/visualization.ipynb): trade-off between expert and grid search approach\n",
    "- Zero effort and no supervision\n",
    "- Grid space reduced on previous iteration's results (mimic expert decisions)\n",
    "- Time consuming (still)\n",
    "- Easy to integrate with sklearn cross validation function. See [examples](https://github.com/fmfn/BayesianOptimization/blob/master/examples/sklearn_example.py) here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Golden rule: finding optimal configuration rarely is a good time investment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Ensembling by voting](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier)\n",
    "```\n",
    "1111111100 = 80% accuracy \n",
    "0111011101 = 70% accuracy \n",
    "1000101111 = 60% accuracy\n",
    "```\n",
    "**Majority Vote**\n",
    "```\n",
    "1111111101 = 90% accuracy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensembling by averaging\n",
    "- Let’s say we have N predictions from N different models: $y_1, y_2, ... , y_N$\n",
    "- We want to make a single prediction using weighted average: $\\beta_1*y_1+\\beta_2*y_2+...+\\beta_N*y_N$\n",
    "- How do we find the best beta cofficients?\n",
    "- Very common mistake to select weights based on leaderboard feedback\n",
    " - **inefficient & prone to leaderboard overfitting**\n",
    "- Solve the problem using CV predictions with optimization algorithms \n",
    " - $optim(\\beta_1*y_1+\\beta_2*y_2+...+\\beta_N*y_N)$ with starting weights $\\beta_i=1/N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stacked Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure for a 5 fold stacking may be described as follows:\n",
    "\n",
    "1. Split the total training set into two disjoint sets (here train and holdout)\n",
    "\n",
    "2. Train several base models on the first part (train)\n",
    "\n",
    "3. Predict these base models on the second part (holdout)\n",
    "\n",
    "4. Repeat step 1-3 five times and use the holdout predictions as the inputs, and the correct responses (target variable) as the outputs to train a higher level learner called meta-model.\n",
    "\n",
    "\n",
    "- For the test set, we could either average the predictions of all base models on the test data or refit the model using the whole training set and then predict. Generally speaking, either way is fine because the test set hasn't seen the training set.\n",
    "- If we ran 10 models using the same procedure, our meta model will have 10 input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://s3.amazonaws.com/nycdsabt01/stacking.jpg)\n",
    "\n",
    "Borrowed from [Faron](https://www.kaggle.com/getting-started/18153#post103381)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick note, one should try a few diverse models. To my experience, a good stacking solution is often composed of at least:\n",
    "- 2 or 3 GBMs/XGBs/LightGBMs (one with low depth, one with medium and one with high)\n",
    "- 1 or 2 Random Forests (again as diverse as possible–one low depth, one high)\n",
    "- 1 linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful if you are debugging the function inside another .py script\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, LinearRegression as lr\n",
    "from sklearn.ensemble import GradientBoostingRegressor as gbr, RandomForestRegressor as rfr\n",
    "from preprocess import impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = impute(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_index = len(X_train)\n",
    "for col in all_data.columns:\n",
    "    if all_data[col].dtype == \"object\":\n",
    "        lce = LabelCountEncoder()\n",
    "        all_data[col][:train_index] = lce.fit_transform(all_data[col][:train_index])\n",
    "        all_data[col][train_index:] = lce.trasnform(all_data[col][train_index:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = all_data.iloc[:train_index, :]\n",
    "X_test = all_data.iloc[train_index:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stacking import stacking_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(np.log(y), np.log(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    # linear model, ElasticNet = lasso + ridge\n",
    "    ElasticNet(random_state=0),\n",
    "    \n",
    "    # conservative random forst model\n",
    "    rfr(random_state=0,\n",
    "        n_estimators=1000, max_depth=6,  max_features='sqrt'),\n",
    "    \n",
    "    # aggressive random forst model\n",
    "    rfr(random_state=0, \n",
    "        n_estimators=1000, max_depth=9,  max_features='auto'),\n",
    "    \n",
    "    # conservative gbm model\n",
    "    gbr(random_state=0, learning_rate = 0.005, max_features='sqrt',\n",
    "        min_samples_leaf=15, min_samples_split=10, \n",
    "        n_estimators=3000, max_depth=3),\n",
    "    \n",
    "    # aggressive gbm model\n",
    "    gbr(random_state = 0, learning_rate = 0.01, max_features='sqrt',\n",
    "        min_samples_leaf=10, min_samples_split=5, \n",
    "        n_estimators = 1000, max_depth = 9)\n",
    "    ]\n",
    "\n",
    "meta_model = lr(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stacking_prediction = stacking_regression(models, meta_model, X_train, y_train, X_test,\n",
    "                               transform_target=np.log1p, transform_pred = np.expm1, \n",
    "                               metric=rmsle, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Having more models than necessary in ensemble may hurt.**\n",
    "\n",
    "Lets say we have a library of created models. Usually greedy-forward approach works well:\n",
    "- Start with a few well-performing models’ ensemble\n",
    "- Loop through each other model in a library and add to current ensemble\n",
    "- Determine best performing ensemble configuration\n",
    "- Repeat until metric converged\n",
    "\n",
    "If you are using linear regression as the meta model, make sure you have **diverse/uncorrelated** first layer models\n",
    "\n",
    "During each loop iteration it is wise to consider only a subset of library models, which could work as a regularization for model selection.\n",
    "\n",
    "Repeating procedure few times and bagging results reduces the possibility of overfitting by doing model selection.\n",
    "\n",
    "Another [great walkthrough](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python) of stacking on Kaggle using the famous Titanic dataset.\n",
    "\n",
    "R users can call the `stackedEnsemble()` function from the [H2o package](https://h2o-release.s3.amazonaws.com/h2o/rel-ueno/2/docs-website/h2o-docs/data-science/stacked-ensembles.html) directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Success formula (personal opinion)\n",
    "\n",
    "50% - feature engineering\n",
    "\n",
    "30% - model diversity\n",
    "\n",
    "10% - luck\n",
    "\n",
    "10% - proper ensembling\n",
    " - Voting\n",
    " - Averaging\n",
    " - Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
